SnakeLLM
Next Steps, Bug Fixes & Fine-Tuning on Google Cloud
Benedict — LLM Core Lead
 
PART A — Immediate Bug Fixes

Fix these before generating more pipelines. They affect the quality of every output.

Fix 1 — featureCounts Container (Critical)

Solution — add alias in llm/execute_rag.py:
Find the _load_tool_registry() method and add these lines:
# In llm/execute_rag.py, _load_tool_registry() method
if 'subread' in self.tool_registry:
    self.tool_registry['featurecounts'] = self.tool_registry['subread']
    self.tool_registry['featureCounts'] = self.tool_registry['subread']

Also manually create data/biocontainers/subread.json with the correct container:
{
  "tool_name": "subread",
  "aliases": ["featureCounts", "featurecounts"],
  "best_container": {
    "registry": "quay.io",
    "image": "biocontainers/subread",
    "tag": "2.0.6--h9a72b6d_0",
    "full_uri": "quay.io/biocontainers/subread:2.0.6--h9a72b6d_0",
    "source": "biocontainers"
  }
}

Fix 2 — DESeq2 Aggregate Rule in Few-Shot Examples (Critical)

Solution — update FEW_SHOT_EXAMPLES in llm/inference.py:
Find the DESeq2 rule in your FEW_SHOT_EXAMPLES and change the input pattern:
# WRONG (what LLM currently generates):
"input": ["counts/{sample}_counts.txt"]

# CORRECT (aggregate — runs once on all samples):
"input": [
    "counts/all_samples_counts.txt"
]

Also add this instruction to EXECUTE_SYSTEM_PROMPT:
# Add to EXECUTE_SYSTEM_PROMPT string:
IMPORTANT: Rules that produce a single output file (no wildcards in output)
must use aggregate inputs. DESeq2, MultiQC, and clusterProfiler are
aggregate rules — their input must reference a merged file or directory,
NOT a {sample} wildcard pattern.

Fix 3 — Fix DAG Edges for FastQC (Quality)

Solution — update FEW_SHOT_EXAMPLES dag_edges:
// WRONG — FastQC blocks alignment:
["post_trim_fastqc", "align_star"]

// CORRECT — FastQC and alignment run in parallel:
["trim_reads", "post_trim_fastqc"],  // QC branch (parallel)
["trim_reads", "align_star"],         // alignment branch (parallel)

Fix 4 — Tool Version Updates in Workflow .md Files (Quality)

Solution — add version guidance to data/workflows/rna_seq_de.md:
Open data/workflows/rna_seq_de.md and add this section at the top:
## Recommended Tool Versions (2024 stable)
- STAR: 2.7.10a (minimum 2.7.x — never use 2.5.x)
- samtools: 1.17+ (never use pre-1.x versions like 0.1.x)
- featureCounts (Subread): 2.0.6
- Trimmomatic: 0.39
- MultiQC: 1.19+
- DESeq2: 1.40.0 (Bioconductor 3.17+)
- clusterProfiler: 4.8.0
- FastQC: 0.12.1

Then re-run setup to re-index:
python main.py setup

Fix 5 — Make Strandedness Configurable

Solution — add to EXECUTE_SYSTEM_PROMPT:
# Add this instruction:
strandedness must always be a config_param, never hardcoded.
Include in config_params:
  "strandedness": 0,  # 0=unstranded, 1=stranded, 2=reverse-stranded
  "paired_end": true  # true for PE, false for SE
The featureCounts -p flag must only be used when paired_end is true.

Fix 6 — Unicode Encoding Error in Windows Terminal

Solution — add to top of main.py (after imports):
import sys
if sys.stdout.encoding != 'utf-8':
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
if sys.stderr.encoding != 'utf-8':
    sys.stderr.reconfigure(encoding='utf-8', errors='replace')

Or set once per PowerShell session:
$env:PYTHONIOENCODING = 'utf-8'

PART B — After Fixes: Run the Benchmark

Once fixes are applied, run these 3 commands to regenerate clean pipelines and start your benchmark data collection:

Step 1 — Regenerate all 3 pipelines
python main.py generate "run differential expression analysis on RNA-seq using DESeq2" --output results/rna_seq_v2.json
python main.py generate "ATAC-seq peak calling with MACS2 and IDR filtering" --output results/atac_seq_v2.json
python main.py generate "WGS variant calling with GATK4 HaplotypeCaller" --output results/wgs_v2.json

Step 2 — Start your benchmark log
Create results/benchmark.csv and record every run:
prompt,pipeline_type,model,schema_pass,rules,tools,notes
"RNA-seq DE DESeq2",rna-seq-de,claude-sonnet-4-6,TRUE,9,8,v2 after fixes
"ATAC-seq MACS2 IDR",atac-seq,claude-sonnet-4-6,TRUE,13,12,v2 after fixes
"WGS GATK4",wgs-variant,claude-sonnet-4-6,TRUE,?,?,v2 after fixes

Step 3 — Send to TM2 for dry-run validation
Hand results/rna_seq_v2.json to TM2. They run:
snakemake --dry-run --cores 1 --snakefile Snakefile
They report pass/fail back to you. That is your first dry-run pass rate data point.

PART C — Converting to ML Research with Fine-Tuning


The Fine-Tuning Research Plan


PART D — Google Cloud Setup

Recommended VM


Step 1 — Create the VM
Option A: Google Cloud Console (easier)
Go to console.cloud.google.com
Compute Engine → VM Instances → Create Instance
Machine type: g2-standard-8
GPU: Add GPU → NVIDIA L4 → Count: 1
Boot disk: Change → pytorch-latest-gpu (Deep Learning on Linux)
Disk size: 100GB
Zone: europe-west4-a
Under Availability: set to Spot (saves 70%)
Create

Option B: gcloud CLI
gcloud compute instances create snakellm-gpu \
  --project=YOUR_PROJECT_ID \
  --zone=europe-west4-a \
  --machine-type=g2-standard-8 \
  --accelerator=type=nvidia-l4,count=1 \
  --image-family=pytorch-latest-gpu \
  --image-project=deeplearning-platform-release \
  --boot-disk-size=100GB \
  --provisioning-model=SPOT \
  --instance-termination-action=STOP \
  --metadata="install-nvidia-driver=True"

Step 2 — Set a Billing Alert First

Step 3 — SSH and Verify GPU
# SSH into the VM
gcloud compute ssh snakellm-gpu --zone=europe-west4-a

# Verify GPU is visible
nvidia-smi
# Should show: NVIDIA L4, 24576MiB VRAM

# Verify PyTorch sees GPU
python -c "import torch; print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0))"
# Should print: True  /  NVIDIA L4

Step 4 — Upload Your Project
# From your local Windows machine
gcloud compute scp -r C:\Users\bened\desktop\snakellm snakellm-gpu:~/ --zone=europe-west4-a

# Or use GCS as intermediate (more reliable for large transfers)
gsutil mb gs://snakellm-data-benedict
gsutil cp -r C:\Users\bened\desktop\snakellm\results gs://snakellm-data-benedict/

# Then on the VM:
gsutil cp -r gs://snakellm-data-benedict/results ~/snakellm/results/

Step 5 — Install Dependencies on VM
pip install peft==0.10.0 trl==0.8.6 bitsandbytes==0.43.0 accelerate==0.29.0 datasets==2.18.0

# Pin versions — these libraries break each other if mismatched
# Verify installation:
python -c "import peft, trl, bitsandbytes; print('OK')"

PART E — Building Your Training Dataset

Do this on your LOCAL machine (no GPU needed). Target: 150 examples minimum, 300+ ideal.

Step 1 — Generate Training Examples
Create generate_training_data.py in your project root:
# generate_training_data.py
import json, os, time
from llm.inference import InferenceEngine
from dotenv import load_dotenv
load_dotenv()

engine = InferenceEngine()

prompts = [
    # --- RNA-seq (50 prompts) ---
    "run differential expression analysis on paired-end RNA-seq using DESeq2",
    "bulk RNA-seq pipeline with edgeR for DE analysis in mouse samples",
    "RNA-seq analysis with STAR alignment and salmon quantification",
    "stranded RNA-seq with Trimmomatic trimming and featureCounts",
    "RNA-seq DE analysis with batch correction using limma-voom",
    "single-end RNA-seq with hisat2 alignment and StringTie quantification",
    "RNA-seq pipeline for zebrafish with DESeq2 and GO enrichment",
    "time-course RNA-seq experiment with DESeq2 LRT test",
    "RNA-seq with UMI deduplication and featureCounts counting",
    "paired RNA-seq with FastQC QC and MultiQC report",
    # add 40 more RNA-seq prompts...

    # --- ATAC-seq (50 prompts) ---
    "ATAC-seq peak calling with MACS2 and IDR reproducibility filtering",
    "ATAC-seq pipeline with Bowtie2 alignment and deepTools QC",
    "open chromatin profiling with ATAC-seq and peak annotation",
    "ATAC-seq differential accessibility analysis with DiffBind",
    # add 46 more ATAC-seq prompts...

    # --- WGS (50 prompts) ---
    "WGS variant calling with GATK4 HaplotypeCaller in GVCF mode",
    "germline SNP calling with BWA-MEM2 and GATK4 best practices",
    "whole genome sequencing with VQSR filtering and Funcotator annotation",
    "WGS pipeline for trio analysis with GATK4 joint genotyping",
    # add 46 more WGS prompts...
]

training_examples = []
os.makedirs('data/training', exist_ok=True)

for i, prompt in enumerate(prompts):
    print(f'Generating {i+1}/{len(prompts)}: {prompt[:60]}...')
    try:
        spec = engine.generate(prompt)
        training_examples.append({
            "instruction": "Generate a complete bioinformatics pipeline as JSON.",
            "input": prompt,
            "output": spec.model_dump_json(indent=2)
        })
        time.sleep(2)  # respect API rate limits
    except Exception as e:
        print(f'  FAILED: {e}')

with open('data/training/pipeline_specs.json', 'w') as f:
    json.dump(training_examples, f, indent=2)

print(f'Generated {len(training_examples)} training examples')
print(f'Estimated cost: ${len(training_examples) * 0.03:.2f}')

Step 2 — Manually Review the Dataset
Before fine-tuning, review every generated example. Remove or fix:
Any example where featureCounts has the wrong container (bedops)
Any example where DESeq2 input uses {sample} wildcard instead of aggregate
Any example where DAG edges force FastQC to block alignment
Any example with obviously wrong tool versions (STAR 2.5.x, samtools 0.1.x)

Quality of training data is more important than quantity. 100 clean examples beat 300 noisy ones.

PART F — The Fine-Tuning Script

Run this on the GCP VM after uploading your training data. Save as finetune.py.

finetune.py — Full Script
import json
from datasets import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer, SFTConfig

MODEL_ID   = 'mistralai/Mistral-7B-Instruct-v0.3'
OUTPUT_DIR = 'models/snakellm-mistral-lora'

# ── Load and format training data ────────────────────────
raw = json.load(open('data/training/pipeline_specs.json'))

def format_example(ex):
    return {
        "text": (
            f"<s>[INST] {ex['instruction']}\n\n{ex['input']} [/INST]\n"
            f"{ex['output']}</s>"
        )
    }

dataset = Dataset.from_list([format_example(ex) for ex in raw])
dataset = dataset.train_test_split(test_size=0.1, seed=42)
print(f'Train: {len(dataset["train"])}  |  Val: {len(dataset["test"])}'

# ── Load model in 4-bit QLoRA ────────────────────────────
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_compute_dtype='float16',
    bnb_4bit_use_double_quant=True,
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,
    device_map='auto',
)

# ── LoRA config — only trains ~0.08% of parameters ──────
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],
    lora_dropout=0.05,
    bias='none',
    task_type='CAUSAL_LM',
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# Output: trainable params: ~6.8M || all params: ~8B (0.08%)

# ── Train ────────────────────────────────────────────────
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset['train'],
    eval_dataset=dataset['test'],
    args=SFTConfig(
        output_dir=OUTPUT_DIR,
        num_train_epochs=3,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        # effective batch size = 8
        learning_rate=2e-4,
        fp16=True,
        logging_steps=10,
        evaluation_strategy='epoch',
        save_strategy='epoch',
        warmup_ratio=0.03,
        lr_scheduler_type='cosine',
        report_to='none',
    ),
    max_seq_length=4096,
    dataset_text_field='text',
)

trainer.train()
trainer.save_model(OUTPUT_DIR)
print(f'Done. Model saved to {OUTPUT_DIR}')

# ── Auto-shutdown VM after training (saves money) ────────
import subprocess
subprocess.run(['sudo', 'shutdown', '-h', 'now'])

Save Model to GCS Before Shutdown

Add this BEFORE the shutdown line in finetune.py:
import subprocess
subprocess.run(['gsutil', '-m', 'cp', '-r',
    'models/snakellm-mistral-lora',
    'gs://snakellm-data-benedict/models/'])
print('Model uploaded to GCS')

PART G — The Benchmark Comparison

After fine-tuning, run the same 30 prompts through 4 systems and measure quality. This becomes Table 1 of your paper.

The 4 Systems to Compare


The 5 Metrics to Measure


Expected Results Table (Paper Table 1)


Note: values above are estimates. Your actual results become the paper's contribution.

PART H — Full Timeline



